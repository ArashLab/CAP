# Tutorials for Cohort Analysis Platform (CAP)

The [examples](examples) folder includes three subsets of [1000-Genome project phase 3 dataset](https://www.internationalgenome.org/data/) all of which contains the full set of 2504 samples in the original dataset. However variants are random subset of different size as below:
* *1kg.micro.vcf.bgz*: 193 variant
* *1kg.tiny.vcf.bgz*: 4053 variant
* *1kg.small.vcf.bgz*: 40408 variant

A simulated phenotypic file (*1kg.pheno.tsv*) is aslo included that present Type2 Diabetes (T2D) and Body Mass Index (BMI) for 100-genome samples along with other real phenotipic data. 

In our [examples](examples) we use the *1kg.micro.vcf.bgz* to make sure everything runs fast but you can use larger subsets too.

The [examples](examples) folder contains a series of numbered example workload in yaml format such as [Example_01.yaml](examples/Example_01.yaml). These workload contains one or a few stages to be executed. The [Example_All.yaml](examples/Example_All.yaml) put all stages toghether.
## Before Running the Examples
It is recomended to copy the [examples](examples) directory into another directory (i.e. temp) as workload files gets updated when you execute them and you loose their initial state.

```bash
git clone https://github.com/ArashLab/CAP.git
cd CAP
cp -r examples temp
cd temp
```

Note that hail is a requierment of cap and pyspark is a reqierment of hail. Thus, when you install cap using pip (as described above) pyspark is also installed on your system. If you haven't had previous spark configuration, the newly installed pyspark execute hail in local mode. Otherwise, you may experience difficulties following these examples.
## Example 1 (Import Genotype from a VCF)‚Äç

The following is a minimal workload that reads a VCF file and write it to a Hail MatrixTable format (with `.mt` extension). The corresponding cap function to do so is **ImportGenotype**. This function requires input and output genotype file (inGT and outGt). Here is the content of workload file. Note that `IGTVCF` is a unique id for this stage.
Note that **ImportGenotype** can read data from different format (currently VCF and Plink Bfiles are supported). It accepts parameters (`arg`) to be passed to the relevant Hail function. See Documentaition for more details. 

```yaml
order:
    - IGTVCF
stages:
    IGTVCF:
        spec:
            function: ImportGenotype
        inout:
            inGt:
                direction: input
                path: 1kg.micro.vcf.bgz
            outGt:
                direction: output
                path: 1kg.ma.mt
```

To execute this workload run the `main` module in the `cap` package with the example workload file.

```bash
python -m cap.main --workload Example_01.yaml
```

Notes:
* If you installed cap in a virtual environemnt, you should activate that environment before runing this above command.
* You must be inside the `temp` directory created above.

The sucessfull compeletion of this workload creates the following files in the current directory.
* *1kg.ma.mt*: The output (Hail MatrixTable is stored in a directory). Since there are multi-alleic site in this dataset we add `ma` to the file name.
* *hail-something.log*: The log file generated by Hail
* *cap.something.log.tsv*: The log file generated by CAP
Note that both log files include date-time element as well as a random string (instead of `something`) to avoid overwriteing older log files.
The CAP log file contains 5 columns as below:
* Date-Time of the log
* Log level
* Current stage when log is generated (if log is not relevant to any stage the value of None is used)
* Corresponding function
* Log message

If you look at the content of the the `Example_01.yaml` file after execution of the workload, you will see information is added to this file.
Some of this information are default values which are added when the workload file is loaded (i.e. `pathType` in the `inGt`) and some others are information that is dynamically added (i.e. `count` in the `outGt`).
The most important one for you to know is the `spec.status` field with is set to `Compeleted`. If you execute the workload once more, it does not execute this stage as it is already marked as `Compeleted`. This feature is quiet handy for the case you have many stages in your workload and it fails at some point. Once you fix the issue, you can execute the workload and make sure only incompleted stages are executed. 

```yaml
order:
- IGTVCF
stages:
    IGTVCF:
        inout:
            inGt:
                compression: bgz
                direction: input
                format: vcf
                path: 1kg.micro.vcf.bgz
                pathType: file
            outGt:
                compression: None
                count:
                    samples: 2504
                    variants: 193
                data: <hail.matrixtable.MatrixTable object at 0x7fa8515552b0>
                direction: output
                format: mt
                isAlive: true
                numPartitions: 4
                path: 1kg.ma.mt
                pathType: file
                toBeCached: true
                toBeCounted: true
        spec:
            endTime: '2021-06-24 11:30:05.825676'
            execTime: '0:00:05.114684'
            function: ImportGenotype
            id: IGTVCF
            startTime: '2021-06-24 11:30:00.710992'
            status: Completed
```

In the successfull compeletion of the workload the following is printed in the terminal. We break this into part and explain them.
```
*** logger is initialised to write to cap.2021-06-24 11:29:53.034782.K1398THY.log.tsv
IGTVCF	CheckStage	Stage is Checked
2021-06-24 11:29:57 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2021-06-24 11:29:58 WARN  Hail:43 - This Hail JAR was compiled for Spark 3.1.1, running with Spark 3.1.2.
  Compatibility is not guaranteed.
Running on Apache Spark version 3.1.2
SparkUI available at http://192.168.1.193:4040
Welcome to
     __  __     <>__
    / /_/ /__  __/ /
   / __  / _ `/ / /
  /_/ /_/\_,_/_/_/   version 0.2.70-5bb98953a4a7
LOGGING: writing to hail-20210624-1129-0.2.70-5bb98953a4a7.log
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	[('spark.hadoop.mapreduce.input.fileinputformat.split.minsize', '0'), ('spark.driver.host', '192.168.1.193'), ('spark.hadoop.io.compression.codecs', 'org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec'), ('spark.ui.showConsoleProgress', 'false'), ('spark.app.startTime', '1624498198227'), ('spark.executor.id', 'driver'), ('spark.logConf', 'true'), ('spark.kryo.registrator', 'is.hail.kryo.HailKryoRegistrator'), ('spark.driver.port', '61401'), ('spark.app.id', 'local-1624498199213'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.kryoserializer.buffer.max', '1g'), ('spark.driver.maxResultSize', '0'), ('spark.executor.extraClassPath', './hail-all-spark.jar'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'Hail'), ('spark.driver.extraClassPath', 'python3.9/site-packages/hail/backend/hail-all-spark.jar')]
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
IGTVCF	CheckStage	Stage is Checked
IGTVCF	ExecuteStage	Started
2021-06-24 11:30:02 Hail: INFO: Coerced sorted dataset
2021-06-24 11:30:03 Hail: INFO: Coerced sorted dataset
2021-06-24 11:30:03 Hail: INFO: Coerced sorted dataset
2021-06-24 11:30:05 Hail: INFO: wrote matrix table with 193 rows and 2504 columns in 4 partitions to 1kg.ma.mt
    Total size: 41.45 KiB
    * Rows/entries: 31.58 KiB
    * Columns: 9.85 KiB
    * Globals: 11.00 B
    * Smallest partition: 42 rows (4.95 KiB)
    * Largest partition:  52 rows (12.08 KiB)
IGTVCF	ExecuteStage	Completed in 0:00:05.114684
```

The following message shows where CAP is going to write its log
```
*** logger is initialised to write to cap.2021-06-24 11:29:53.034782.K1398THY.log.tsv
```

The workload file is loaded and all stages are checked for errors. In this case, we only have one stage to be checked
```
IGTVCF	CheckStage	Stage is Checked
```

Some Spark stuff.
```
2021-06-24 11:29:57 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2021-06-24 11:29:58 WARN  Hail:43 - This Hail JAR was compiled for Spark 3.1.1, running with Spark 3.1.2.
  Compatibility is not guaranteed.
Running on Apache Spark version 3.1.2
SparkUI available at http://192.168.1.193:4040
```

Hail initialization and where hail log file is stored
```
Welcome to
     __  __     <>__
    / /_/ /__  __/ /
   / __  / _ `/ / /
  /_/ /_/\_,_/_/_/   version 0.2.70-5bb98953a4a7
LOGGING: writing to hail-20210624-1129-0.2.70-5bb98953a4a7.log
```

We print out the spark configuration used in the process and highlight it with many + signs.
Please note the `('spark.master', 'local[*]')` and `('spark.submit.deployMode', 'client')`
```
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	[('spark.hadoop.mapreduce.input.fileinputformat.split.minsize', '0'), ('spark.driver.host', '192.168.1.193'), ('spark.hadoop.io.compression.codecs', 'org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec'), ('spark.ui.showConsoleProgress', 'false'), ('spark.app.startTime', '1624498198227'), ('spark.executor.id', 'driver'), ('spark.logConf', 'true'), ('spark.kryo.registrator', 'is.hail.kryo.HailKryoRegistrator'), ('spark.driver.port', '61401'), ('spark.app.id', 'local-1624498199213'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.kryoserializer.buffer.max', '1g'), ('spark.driver.maxResultSize', '0'), ('spark.executor.extraClassPath', './hail-all-spark.jar'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'Hail'), ('spark.driver.extraClassPath', 'python3.9/site-packages/hail/backend/hail-all-spark.jar')]
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
None	__init__	+++++++++++++++++++++++++++++++
```

Each stage is being check once again right before execution. Everything between `IGTVCF	ExecuteStage	Started` and `IGTVCF	ExecuteStage	Completed in 0:00:05.114684` are messages printed by hail.
```
IGTVCF	CheckStage	Stage is Checked
IGTVCF	ExecuteStage	Started
2021-06-24 11:30:02 Hail: INFO: Coerced sorted dataset
2021-06-24 11:30:03 Hail: INFO: Coerced sorted dataset
2021-06-24 11:30:03 Hail: INFO: Coerced sorted dataset
2021-06-24 11:30:05 Hail: INFO: wrote matrix table with 193 rows and 2504 columns in 4 partitions to 1kg.ma.mt
    Total size: 41.45 KiB
    * Rows/entries: 31.58 KiB
    * Columns: 9.85 KiB
    * Globals: 11.00 B
    * Smallest partition: 42 rows (4.95 KiB)
    * Largest partition:  52 rows (12.08 KiB)
IGTVCF	ExecuteStage	Completed in 0:00:05.114684
```


## Example 2 (Break Multi-Allelic Loci)

The **SplitMulti** function in CAP help to break multi-allleic site to multiple bi-alleleic site. In this example, we pass the `withHTS` argument to the function too. `ba` in the output file name referes to bi-allelic.


```yaml
order:
    - BRKMA
stages:
    BRKMA:
        spec:
            function: SplitMulti
        arg:
            withHTS: true
        inout:
            inGt:
                direction: input
                path: 1kg.ma.mt
            outGt:
                direction: output
                path: 1kg.ba.mt
```

The updated workload file after this example show that the number of loci increased by 2 (from 193 to 195).

```yaml
stages:
    BRKMA:
        inout:
            inGt:
                count:
                    variants: 193
            outGt:
                count:
                    variants: 195
```

## Example 3 (Multiple Stages in One workload)

The following example shows how to put two stages in one workload. Note that the order of stage ids in the `order` field is important (they are executed in the same order) but you don't need to order the definition of stages. Also we define `mtPathMa` to avoid replication of the file name across our workload file.

Note that `1kg.ma.mt` is the output of `IGTVCF` and the input to the `BRKMA`. When you run this workflow, CAP keeps the `1kg.ma.mt` alive in the memory to move through stages.

```yaml
config:
    mtPathMa: &mtPathMa 1kg.ma.mt
order:
    - IGTVCF
    - BRKMA
stages:
    IGTVCF:
        spec:
            function: ImportGenotype
        inout:
            inGt:
                direction: input
                path: 1kg.micro.vcf.bgz
            outGt:
                direction: output
                path: *mtPathMa
    BRKMA:
        spec:
            function: SplitMulti
        arg:
            withHTS: true
        inout:
            inGt:
                direction: input
                path: *mtPathMa
            outGt:
                direction: output
                path: 1kg.ba.mt
```

If you execute this workload after you execute example 1 and 2, you will get and exception error message as follow. When CAP checks stages, it checks that output files does not exist on the system to prevent overwriting. Since example 1 already produced `1kg.ma.mt` you will get this error.

```
Exception: IGTVCF	CheckInout	<< inout: outGt >> Output path (or plink bfile prefix) /Users/arashbayat/newCAP/CAP/temp/1kg.ma.mt already exist in the file system
```

If you insist to run this workload you need to delete output files first
```bash
rm -rf *.mt
python -m cap.main --workload Example_03.yaml
```

## Example 4 (Add Numeric Ids)

Most of CAP functions requiers a uniqe and numeric sample and variant ids (`sampleId` and `variantId`). These ids are used to connect all tables together. Also, depending on the source file where genotypes are loaded, there could be annotations for samples and variants loaded into the matrix table. The **AddId** function in CAP adds the numeric Ids and also extract all sample and variant annotations and export them in separate HailTbale (with `.ht` extension).

```yaml
order:
    - ADDID
stages:
    ADDID:
        spec:
            function: AddId
        inout:
            inGt:
                direction: input
                path: 1kg.ba.mt
            outGt:
                direction: output
                path: 1kg.mt
            outCol:
                direction: output
                path: 1kg.samples.ht
            outRow:
                direction: output
                path: 1kg.variants.ht
```

## Example 5 (Vriant Annotations to TSV)

1000-Genome VCF file which we used in our example includes several variant annotations. The **AddId** function export them into `1kg.variants.ht` files and now we would like to show you how to export these information into a TSV file. 

```yaml
order:
    - TSVVAR
stages:
    TSVVAR:
        spec:
            function: ToText
        arg:
            header: true
            delimiter: "\t"
        inout:
            inHt:
                direction: input
                path: 1kg.variants.ht
            outText:
                direction: output
                path: 1kg.variants.tsv
```

Here is a few first line from `1kg.variants.tsv`:

```
locus.contig	locus.position	rsid	qual	a_index	was_split	variantId	alleles.1	alleles.2
1	20788154	rs557428684	1.0000e+02	1	false	1	A	G
1	36434303	rs574016683	1.0000e+02	1	false	2	T	C
1	36905328	rs149158928	1.0000e+02	1	false	3	A	G
1	40566531	rs538135875	1.0000e+02	1	false	4	T	C
1	61171864	rs544695542	1.0000e+02	1	false	5	A	C
1	70114929	rs190757654	1.0000e+02	1	false	6	A	T
1	86500526	rs375700092	1.0000e+02	1	false	7	T	G
1	93992857	rs184594653	1.0000e+02	1	false	8	C	T
1	97156615	rs528557531	1.0000e+02	1	false	9	A	G
```
## Example 6 (All together)

This example include the following stages:
* `IGTVCF`: Import GenoType from VCF file
* `BRKMA`: Break multi-allelic loci
* `ADDID`: Add numeric ids
* `TSVSAM`: Export sample information in TSV format
* `TSVVAR`: Export variant information in TSV format
* `EGTVCF`: Export Genotype (output of `ADDID`) in VCF format
* `EGTPLINK`: Export Genotype (output of `ADDID`) in plink2 bfile format
* `IPT`: Import Phenotype form TSV file. This stage also requiers `1kg.samples.ht` that is the output of `ADDID` to link phenotype with the numerical sample ids. 
* `TSVPHENO`: Export Phenotype in TSV format. This is as same as input phenotype but also include the numerical sample ids
* `PCA`: Perform Principle Component Analysis (PCA) and compute scores and loading. It also output list of variants included in the calculation. The PCA function allows to sub-sampleing data.
* `TSVPCASCORE`: Export PCA scores in TSV format
* `TSVPCAVAR`: Export list of variant used in PCA calculation in TSV format.
* `SQC`: Calculate Quality Control (QC) metrics for samples
* `VQC`: Calculate Quality Control (QC) metrics for variants
* `TSVSQC`: Export sample QC in TSV format
* `TSVVQC`: Export variant QC in TSV format

To run this example start from a fresh copy of the [examples](examples) folder. 
